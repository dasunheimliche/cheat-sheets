## A - Análisis Asintótico (Asymptotic Analysis)

#### 1. **Definición:**

El **análisis asintótico** es como usar una lupa para ver cómo se comporta un algoritmo cuando el tamaño del problema (normalmente lo llamamos 'n') se hace ¡enorme! Imagina que 'n' es la cantidad de datos que le das a tu algoritmo. Lo que nos interesa es saber qué tan rápido o lento se vuelve el algoritmo a medida que 'n' crece y crece, ¡hasta el infinito y más allá! En el análisis asintótico, nos enfocamos en la **tasa de crecimiento** del algoritmo y solemos ignorar los pequeños detalles como las constantes o los términos menos importantes.

#### 2. **Ejemplo:**

Mira estas gráficas. Imagina que representan el tiempo que tardan diferentes algoritmos en función del tamaño de la entrada 'n'.

![image](https://opendsa-server.cs.vt.edu/ODSA/Books/Everything/html/_images/GrowthRates1.png)

**Explicación del ejemplo:**

- Aunque la línea `10n` empieza más arriba que `2n^2` (mira la primera gráfica), si te fijas en la segunda gráfica (que es un zoom de la parte inferior izquierda de la primera), verás que en algún punto, la curva `2n^2` ¡cruza y supera a `10n`! Esto significa que para tamaños de 'n' grandes, el algoritmo que se comporta como `2n^2` se vuelve mucho más lento que el que se comporta como `10n`.
- Incluso si cambiamos el `10n` a `20n`, ¡la curva `2n^2` eventualmente lo superará también! Lo importante aquí es la _forma_ de la curva, no tanto si empieza un poco más arriba o abajo.

#### 3. **Notas o advertencias:**

- **¿Por qué ignorar las constantes?** Porque cuando 'n' se hace muy grande, las constantes se vuelven insignificantes comparadas con la forma en que crece la función (n, n², log n, etc.). Es como si compararas el peso de una hormiga con el de un elefante: ¡la hormiga no importa mucho!
- **Ojo con los casos especiales:** A veces, para problemas muy pequeños, las constantes sí importan. Si siempre vas a trabajar con 'n' pequeño (por ejemplo, ordenar listas de 5 elementos), un algoritmo con una constante pequeña pero peor tasa de crecimiento podría ser más rápido en la práctica. ¡Pero esto es raro!

## B - Cota Superior y Notación Big-O (Upper Bounds and Big-O Notation)

#### 1. **Definición:**

La **cota superior** de un algoritmo nos dice, en el peor de los casos, ¡cuánto podría tardar en ejecutarse! Es como decir: "Tranquilo, ¡nunca va a tardar más de esto!". Para expresar estas cotas superiores de forma estándar, usamos la **notación Big-O**.

#### 2. **Ejemplo:**

Si decimos que un algoritmo está en **O(n²)** (se lee "O de n cuadrado"), significa que en el peor de los casos, el tiempo de ejecución del algoritmo crecerá _como mucho_ de forma cuadrática con respecto al tamaño de la entrada 'n'. Podría crecer más lento, ¡pero nunca más rápido que n²!

**Formalmente:**

Decimos que una función T(n) está en **O(f(n))** si existen dos constantes positivas, 'c' y 'n₀', tales que para todo 'n' mayor que 'n₀', se cumple que:

**T(n) ≤ c \* f(n)**

- **T(n):** Es el tiempo de ejecución real de tu algoritmo.
- **f(n):** Es la función que representa la cota superior (ej: n, n², log n, etc.).
- **c:** Es una constante positiva. ¡No importa su valor exacto!
- **n₀:** Es un valor de 'n' a partir del cual la cota superior empieza a cumplirse. Normalmente es un número pequeño.

**En cristiano:** A partir de cierto tamaño de entrada (n₀), el tiempo de tu algoritmo (T(n)) siempre será menor o igual a alguna constante (c) multiplicada por f(n).

#### 3. **Ejemplos concretos:**

- **Búsqueda Secuencial (Sequential Search):** Si buscas un número en una lista uno por uno, en el peor de los casos (que el número esté al final o no esté), tendrás que revisar todos los 'n' elementos. Por eso, la búsqueda secuencial es **O(n)**.

  **Ejemplo de código (pseudocódigo):**

  ```
  funcion busquedaSecuencial(lista, valor):
      para cada elemento en lista:
          si elemento == valor:
              retornar Verdadero // Encontrado
      retornar Falso // No encontrado
  ```

- **Algoritmo con tiempo T(n) = c₁n² + c₂n:** Este algoritmo está en **O(n²)**. ¿Por qué? Porque para 'n' grandes, el término 'n²' domina sobre 'n'. Podemos encontrar una constante 'c' (por ejemplo, c = c₁ + c₂) tal que c₁n² + c₂n ≤ cn² para todo 'n' mayor que 1.

- **Asignar un valor de un array a una variable:** Esto toma un tiempo **constante**, ¡no importa el tamaño del array! En notación Big-O, decimos que es **O(1)**.

  **Ejemplo de código (pseudocódigo):**

  ```
  variable = lista[0] // Acceder al primer elemento
  ```

#### 4. **Notas o advertencias:**

- **"¿Mejor en qué caso?"** Siempre que hables de Big-O, debes especificar si es en el mejor caso, caso promedio o peor caso. Normalmente, cuando se dice "Big-O de un algoritmo", se refiere al **peor caso**.
- **O(n) también está en O(n²):** Si un algoritmo es O(n), ¡también es correcto decir que es O(n²), O(n³), etc.! Porque n crece más lento que n² y n³. Pero siempre buscamos la **cota superior más ajustada**, la más "pequeña" posible. Por eso, decimos que la búsqueda secuencial es O(n), ¡y no O(n²)!
- **"∈ O(f(n))" vs "= O(f(n))":** Es más correcto decir "está en O(f(n))" o usar el símbolo "∈ O(f(n))" en lugar de "= O(f(n))". Big-O define un _conjunto_ de funciones que crecen como máximo a cierto ritmo.

## C - Reglas de Simplificación para Big-O (Simplifying Rules)

#### 1. **Regla 1: Transitividad**

Si f(n) es O(g(n)) y g(n) es O(h(n)), entonces f(n) es O(h(n)).

**En cristiano:** Si algo está acotado por algo más grande, y eso más grande está acotado por algo aún más grande, ¡entonces lo primero también está acotado por lo más grande! Es como una cadena de "es más pequeño que".

#### 2. **Regla 2: Constantes Multiplicativas**

Si f(n) es O(k \* g(n)) donde 'k' es cualquier constante positiva, entonces f(n) es O(g(n)).

**En cristiano:** ¡Las constantes multiplicativas no importan en Big-O! O(10n) es lo mismo que O(n), O(0.5n²) es lo mismo que O(n²). Simplemente nos quedamos con la función principal (n, n², log n, etc.).

#### 3. **Regla 3: Suma**

Si tienes dos partes de un algoritmo que se ejecutan una después de la otra, y la primera parte es O(g₁(n)) y la segunda es O(g₂(n)), entonces el tiempo total del algoritmo es O(max(g₁(n), g₂(n))).

**En cristiano:** Cuando sumas tiempos, ¡solo te quedas con el término más grande! Si tienes una parte que tarda O(n) y otra que tarda O(n²), el tiempo total es O(n²). La parte O(n) se vuelve insignificante para 'n' grandes.

#### 4. **Regla 4: Producto**

Si tienes un bucle donde repites una operación, y la operación es O(g₁(n)) y el bucle se repite O(g₂(n)) veces, entonces el tiempo total del bucle es O(g₁(n) \* g₂(n)).

**En cristiano:** Para bucles simples, multiplicas el tiempo de lo que hay dentro del bucle por el número de veces que se repite el bucle. Por ejemplo, si tienes un bucle que se ejecuta 'n' veces y dentro haces algo que tarda O(n), el tiempo total es O(n \* n) = O(n²).

#### 5. **Notas o advertencias:**

- **Ignorar constantes y términos de menor orden:** Gracias a estas reglas, para calcular el Big-O de un algoritmo, ¡puedes simplificar mucho! Ignora todas las constantes multiplicativas, y en las sumas, quédate solo con el término de mayor orden (el que crece más rápido). Por ejemplo, si T(n) = 5n³ + 2n² + 10n + 7, ¡entonces T(n) es O(n³)! Los términos 2n², 10n y 7 no importan para 'n' grandes.
