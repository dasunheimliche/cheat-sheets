## A - Lower Bounds (Límites Inferiores) - Notación Omega (Ω)

#### 1. **Definición:**

En análisis de algoritmos, un **límite inferior** (lower bound) nos dice la **menor cantidad de recursos** (normalmente tiempo) que un algoritmo necesita, **como mínimo**, para resolver un problema con una entrada de tamaño `n`. Piensa en ello como el "piso" de rendimiento de tu algoritmo.

La notación **Omega (Ω)**, que se pronuncia "Big-Omega" u "Omega", es la que usamos para expresar este límite inferior. Es como la hermana de la notación Big-Oh, pero en lugar de hablar del "techo" (límite superior), hablamos del "piso" (límite inferior).

Formalmente, decimos que una función de tiempo `T(n)` está en **Ω(g(n))** si, a partir de un cierto tamaño de entrada `n0`, `T(n)` siempre es mayor o igual a `c * g(n)`, donde `c` es una constante positiva. En cristiano: **a partir de cierto punto, tu algoritmo siempre tardará al menos un tiempo proporcional a `g(n)`**.

#### 2. **Ejemplo:**

Imagina que tienes un algoritmo con un tiempo de ejecución descrito por: `T(n) = c1*n^2 + c2*n`, donde `c1` y `c2` son números positivos.

**Demostración de que T(n) es Ω(n²):**

Podemos simplificar esto para encontrar un límite inferior. Para cualquier `n > 1`, sabemos que `c1*n^2 + c2*n` siempre será mayor o igual que `c1*n^2` (porque `c2*n` es positivo).

Así que, podemos decir que `T(n) ≥ c * n^2`, donde `c = c1` y `n0 = 1`.

**Conclusión:** Según la definición de Omega, ¡`T(n)` está en **Ω(n²)**! Esto significa que el tiempo de ejecución de este algoritmo crece al menos tan rápido como `n²`.

**Explicación del ejemplo:**
Lo que hicimos aquí fue encontrar una función más simple (`n²`) que siempre sea menor o igual a nuestro tiempo de ejecución original (`c1*n^2 + c2*n`), al menos a partir de cierto punto (`n0 = 1`). Esto nos permite decir que `n²` es un límite inferior para nuestro algoritmo.

#### 3. **Notas o advertencias:**

- **"Tightest" Bound (Límite más "ajustado"):** Al igual que con Big-Oh, queremos el límite Omega más "ajustado" posible, pero en este caso, ¡el más **grande**! Aunque `T(n) = c1*n^2 + c2*n` también está en Ω(n), decir que está en Ω(n²) es más informativo porque `n²` crece más rápido que `n`, dándonos una idea más precisa del "piso" de rendimiento.

- **Búsqueda Secuencial:** Recuerda el algoritmo de búsqueda secuencial en un array. En el peor y en el caso promedio, es Ω(n). ¿Por qué? Porque en ambos casos, ¡necesitas examinar **al menos** una cantidad de elementos que es proporcional a `n`! (en el peor caso, todos; en el promedio, la mitad aproximadamente).

- **No confundir con el mejor caso:** Es fácil confundir el límite inferior (Ω) con el mejor caso. **No son lo mismo.** Omega habla del mínimo rendimiento **para cualquier entrada de tamaño n** (en el contexto que estemos analizando: peor, promedio o mejor caso). El mejor caso es un escenario específico, no una cota general.

## B - Definición Alternativa (¡Ojo, no equivalente!) de Omega (Ω)

#### 1. **Definición:**

Existe otra definición de Omega que **no es exactamente igual** a la que vimos antes, aunque a veces puede ser útil. Según esta definición:

`T(n)` está en **Ω(g(n))** si existe una constante positiva `c` tal que `T(n) ≥ c * g(n)` **para un número infinito de valores de `n`**.

**¡Atención!** Esta definición es más "relajada". No exige que `T(n) ≥ c * g(n)` se cumpla para _todos_ los `n` mayores que un `n0`, sino solo para **muchos** valores de `n` (infinitos, de hecho).

#### 2. **Ejemplo:**

Considera un algoritmo con este comportamiento "raro":

```
T(n) = {
    n   , si n es impar y n ≥ 1
    n²/100, si n es par y n ≥ 0
}
```

Para los valores **pares** de `n`, `n²/100 ≥ (1/100) * n²`. Así que, para **infinitos** valores de `n` (todos los pares), `T(n) ≥ c * n²` con `c = 1/100`.

**Conclusión (con la definición alternativa):** ¡`T(n)` está en **Ω(n²)**!

**Explicación del ejemplo:**
Aunque para los números impares el tiempo de ejecución es solo `n`, para una cantidad infinita de entradas (los números pares), el tiempo es `n²` (dividido por 100, pero sigue siendo cuadrático). La definición alternativa de Omega nos permite capturar este comportamiento y decir que, en cierto sentido, el algoritmo puede ser tan lento como `n²`.

#### 3. **Notas o advertencias:**

- **¿Por qué esta definición alternativa?** La primera definición de Omega (la más común) podría no ser suficiente para describir algoritmos con comportamientos muy irregulares como este ejemplo. Con la primera definición, solo podríamos decir que este algoritmo es Ω(n), lo cual no refleja que a veces puede ser mucho más lento (`n²`). La definición alternativa nos da una descripción más "sensata" en estos casos extraños.

- **Comportamiento Patológico:** Afortunadamente, la mayoría de los algoritmos "reales" no se comportan de forma tan extraña. Por lo general, la primera definición de Omega es suficiente y más intuitiva.

- **Herramienta de Modelado:** Recuerda que la notación asintótica (Big-Oh, Omega, Theta) no es una ley física, ¡es una herramienta para describir y comparar algoritmos!

## C - Theta Notation (Notación Theta - Θ)

#### 1. **Definición:**

La notación **Theta (Θ)**, o "Big-Theta", la usamos cuando un algoritmo tiene **el mismo límite superior e inferior**, ¡dentro de un factor constante! Es decir, un algoritmo es **Θ(h(n))** si es **tanto O(h(n)) como Ω(h(n))**.

En palabras más sencillas: **Theta significa que el tiempo de ejecución de tu algoritmo crece exactamente al mismo ritmo que `h(n)`, ni más rápido, ni más lento (asintóticamente hablando)**. Es como decir que el "piso" y el "techo" de rendimiento están en el mismo lugar.

Cuando decimos que algo es Θ(h(n)), usamos el signo de igualdad (=) en lugar de "está en el conjunto". Por ejemplo, si `f(n)` es Θ(g(n)), entonces `g(n)` también es Θ(f(n)). ¡Es una relación simétrica!

#### 2. **Ejemplo:**

Volvamos a la **búsqueda secuencial**. Vimos que en el caso promedio es **O(n)** (límite superior) y **Ω(n)** (límite inferior). Como ambos límites son `n`, ¡podemos decir que la búsqueda secuencial en el caso promedio es **Θ(n)**!

**Explicación del ejemplo:**
Theta nos da una descripción mucho más precisa del rendimiento de la búsqueda secuencial en el caso promedio. No solo sabemos que no es peor que `n` (Big-Oh) ni mejor que `n` (Omega), sino que **es exactamente del orden de `n`**.

#### 3. **Notas o advertencias:**

- **Análisis "Perfecto":** Cuando tienes una ecuación algebraica que describe el tiempo de ejecución de un algoritmo, ¡los límites superior e inferior siempre se "encuentran"! Esto significa que, en teoría, siempre puedes dar una notación Theta para algoritmos bien definidos.

- **Algoritmos Comunes:** Para muchos algoritmos que usamos a diario, el análisis es bien conocido y podemos dar una notación Theta.

- **Problemas Difíciles (NP-Completos):** Sin embargo, hay problemas muy complicados, como los problemas **NP-Completos**, para los que **no tenemos un análisis Theta definitivo**. Solo podemos dar límites Big-Oh y Omega que no coinciden.

- **Ejemplo Misterioso (Conjetura de Collatz):** Incluso programas aparentemente simples pueden ser difíciles de analizar. El ejemplo del código que mencionan (la conjetura de Collatz) es famoso porque **nadie sabe los límites superior e inferior verdaderos** de su tiempo de ejecución. ¡Es un problema abierto en matemáticas e informática!

- **Preferir Theta a Big-Oh:** Aunque a veces se usa "Big-Oh" de forma informal para hablar del orden de un algoritmo, **es mejor usar Theta cuando realmente sabes que los límites superior e inferior coinciden**. En general, es más preciso y da una imagen más completa del rendimiento. Este material (OpenDSA) prefiere usar Theta siempre que sea posible. Big-Oh y Omega se reservan para casos donde solo conocemos un límite o cuando hablamos específicamente de límites superiores o inferiores.

## D - Classifying Functions (Clasificando Funciones) - Comparando Tasas de Crecimiento

#### 1. **Definición:**

Si tienes dos funciones, `f(n)` y `g(n)`, y quieres saber cuál crece más rápido a medida que `n` se hace muy grande, la mejor manera es usar **límites**. Calcula este límite:

```
lim (n→∞)  f(n) / g(n)
```

Analiza el resultado de este límite:

- **Si el límite es ∞ (infinito):** Significa que `f(n)` crece **mucho más rápido** que `g(n)`. En notación asintótica, esto quiere decir que `f(n)` está en **Ω(g(n))** (¡y también que `g(n)` está en **O(f(n))**!).

- **Si el límite es 0 (cero):** Significa que `g(n)` crece **mucho más rápido** que `f(n)`. En notación asintótica, esto quiere decir que `f(n)` está en **O(g(n))**.

- **Si el límite es un número constante diferente de cero:** Significa que `f(n)` y `g(n)` crecen **al mismo ritmo**. En notación asintótica, esto quiere decir que `f(n)` es **Θ(g(n))** (y también que `g(n)` es **Θ(f(n))**!).

#### 2. **Ejemplo:**

Comparemos `f(n) = n²` y `g(n) = 2n log n`. ¿`f(n)` es O(g(n)), Ω(g(n)), o Θ(g(n))?

Calculamos el límite:

```
lim (n→∞)  n² / (2n log n)  =  lim (n→∞)  n / (2 log n)
```

Si aplicas la regla de L'Hôpital (o simplemente sabes cómo crecen las funciones), verás que este límite tiende a **∞ (infinito)**. ¿Por qué? Porque `n` crece mucho más rápido que `log n`.

**Conclusión:** Como el límite es infinito, `n²` está en **Ω(2n log n)**. Es decir, `n²` crece más rápido que `2n log n`.

**Explicación del ejemplo:**
Usar límites nos da una forma matemática precisa de comparar la "velocidad" de crecimiento de las funciones. En este caso, el límite infinito nos confirma que `n²` domina a `2n log n` a medida que `n` se hace grande.

## E - Imagen Explicativa: Lower Bounds y Casos

Aquí tienes la imagen que mencionaste, ¡vamos a entenderla!

![Proficient](https://opendsa-server.cs.vt.edu/ODSA/Books/Everything/html/_static/Images/green_check.png) ![Error Saving](https://opendsa-server.cs.vt.edu/ODSA/Books/Everything/html/_static/Images/warning.png)

La imagen muestra un ejemplo de búsqueda de un valor `K` en un array. Vamos a analizar los límites inferiores (Omega) en diferentes casos:

- **Mejor Caso: Ω(1)** - En el mejor de los casos, ¡el valor `K` que buscas está justo en la primera posición del array! Solo necesitas hacer **una** comparación. Por eso, el límite inferior en el mejor caso es **Ω(1)** (tiempo constante). Siempre harás al menos una operación.

- **Peor Caso: Ω(n)** - En el peor caso, el valor `K` está al final del array, o ¡no está en el array! En este caso, podrías tener que revisar **todos** los `n` elementos del array. Por lo tanto, el límite inferior en el peor caso es **Ω(n)** (tiempo lineal). Siempre tendrás que hacer al menos un número de operaciones proporcional a `n` en el peor escenario.

- **Caso Promedio: Ω(n)** - En el caso promedio, asumiendo que el valor `K` tiene la misma probabilidad de estar en cualquier posición (o no estar), en promedio tendrás que revisar aproximadamente la mitad del array. Aunque sea la mitad, sigue siendo proporcional a `n`. Así que, el límite inferior en el caso promedio también es **Ω(n)** (tiempo lineal). En promedio, siempre harás al menos un número de operaciones proporcional a `n`.

**En resumen:** La imagen ilustra cómo el límite inferior (Omega) puede variar dependiendo del caso que estés analizando (mejor, peor, promedio), pero siempre te da una idea del **mínimo** trabajo que el algoritmo debe realizar.
